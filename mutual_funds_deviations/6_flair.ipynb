{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from utils import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import flair"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "flair_sentiment = flair.models.TextClassifier.load('en-sentiment')\n",
    "s = flair.data.Sentence(txt)\n",
    "flair_sentiment.predict(s)\n",
    "total_sentiment = s.labels\n",
    "total_sentiment\n",
    "s.tag, s.score\n",
    "flair_sentiment.predict(s);\n",
    "s.labels\n",
    "# from flair.data_fetcher import NLPTask\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "sec_corpus = flair.datasets.NER_ENGLISH_SEC_FILLINGS()\n",
    "print(sec_corpus.obtain_statistics())\n",
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "                   FlairEmbeddings('news-forward'),\n",
    "                   FlairEmbeddings('news-backward'),\n",
    "                   ]\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings  #, Sentence\n",
    "\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "flair_embedding_backward = FlairEmbeddings('news-backward')\n",
    "\n",
    "document_embeddings = DocumentPoolEmbeddings([glove_embedding,\n",
    "                                              flair_embedding_backward,\n",
    "                                              flair_embedding_backward],\n",
    "                                             mode='min')\n",
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                                   hidden_size=512,\n",
    "                                                                   reproject_words=True,\n",
    "                                                                   reproject_words_dimension=256,\n",
    "                                                                   )\n",
    "\n",
    "classifier = TextClassifier(document_embeddings, label_type=, multi_label=False)\n",
    "\n",
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "document_embeddings = DocumentRNNEmbeddings([glove_embedding])\n",
    "sentence = Sentence('The grass is green . And the sky is blue .')\n",
    "\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "print(sentence.get_embedding())\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from flair.datasets import UD_ENGLISH\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 19:59:30,852 Reading data from /Users/zad/.flair/datasets/ner_english_sec_fillings\n",
      "2022-05-30 19:59:30,853 Train: /Users/zad/.flair/datasets/ner_english_sec_fillings/FIN5.txt\n",
      "2022-05-30 19:59:30,854 Dev: None\n",
      "2022-05-30 19:59:30,855 Test: /Users/zad/.flair/datasets/ner_english_sec_fillings/FIN3.txt\n"
     ]
    }
   ],
   "source": [
    "corpus = flair.datasets.NER_ENGLISH_SEC_FILLINGS()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "label_type = 'pos'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:10:39,500 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1051it [00:00, 7905.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:10:39,637 Dictionary created for label 'pos' with 42 values: NNP (seen 5463 times), NN (seen 4962 times), IN (seen 4239 times), DT (seen 3761 times), CC (seen 2055 times), NNS (seen 2039 times), , (seen 1866 times), JJ (seen 1706 times), CD (seen 1349 times), : (seen 1240 times), . (seen 1218 times), VB (seen 925 times), VBN (seen 915 times), TO (seen 885 times), VBZ (seen 678 times), RB (seen 613 times), MD (seen 440 times), VBG (seen 430 times), VBD (seen 330 times), VBP (seen 291 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_dict = corpus.make_label_dictionary(label_type=label_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 4. initialize embeddings\n",
    "embedding_types = [\n",
    "\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings\n",
    "    # FlairEmbeddings('news-forward'),\n",
    "    # FlairEmbeddings('news-backward'),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "embeddings = StackedEmbeddings(embeddings=embedding_types)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'document_embeddings' and 'label_type'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-50-be04db391b00>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# 5. initialize sequence tagger\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtagger\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextClassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhidden_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m256\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m                         \u001B[0;31m# embeddings=embeddings,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m                         \u001B[0;31m# tag_dictionary=label_dict,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                         \u001B[0;31m# tag_type=label_type,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() missing 2 required positional arguments: 'document_embeddings' and 'label_type'"
     ]
    }
   ],
   "source": [
    "# 5. initialize sequence tagger\n",
    "tagger = TextClassifier(hidden_size=256,\n",
    "                        # embeddings=embeddings,\n",
    "                        # tag_dictionary=label_dict,\n",
    "                        # tag_type=label_type,\n",
    "                        # use_crf=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:11:32,340 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:11:32,341 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings(\n",
      "      'glove'\n",
      "      (embedding): Embedding(400001, 100)\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=44, bias=True)\n",
      "  (loss_function): ViterbiLoss()\n",
      "  (crf): CRF()\n",
      ")\"\n",
      "2022-05-30 20:11:32,342 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:11:32,343 Corpus: \"Corpus: 1051 train + 117 dev + 305 test sentences\"\n",
      "2022-05-30 20:11:32,344 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:11:32,345 Parameters:\n",
      "2022-05-30 20:11:32,346  - learning_rate: \"0.100000\"\n",
      "2022-05-30 20:11:32,347  - mini_batch_size: \"32\"\n",
      "2022-05-30 20:11:32,348  - patience: \"3\"\n",
      "2022-05-30 20:11:32,349  - anneal_factor: \"0.5\"\n",
      "2022-05-30 20:11:32,352  - max_epochs: \"10\"\n",
      "2022-05-30 20:11:32,355  - shuffle: \"True\"\n",
      "2022-05-30 20:11:32,356  - train_with_dev: \"False\"\n",
      "2022-05-30 20:11:32,357  - batch_growth_annealing: \"False\"\n",
      "2022-05-30 20:11:32,359 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:11:32,366 Model training base path: \"resources/taggers/example-upos\"\n",
      "2022-05-30 20:11:32,368 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:11:32,370 Device: cpu\n",
      "2022-05-30 20:11:32,372 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:11:32,376 Embeddings storage mode: cpu\n",
      "2022-05-30 20:11:32,379 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:11:38,003 epoch 1 - iter 3/33 - loss 4.07447703 - samples/sec: 17.09 - lr: 0.100000\n",
      "2022-05-30 20:11:49,119 epoch 1 - iter 6/33 - loss 3.73019140 - samples/sec: 8.64 - lr: 0.100000\n",
      "2022-05-30 20:11:53,448 epoch 1 - iter 9/33 - loss 3.58966484 - samples/sec: 22.19 - lr: 0.100000\n",
      "2022-05-30 20:12:00,940 epoch 1 - iter 12/33 - loss 3.46719963 - samples/sec: 12.82 - lr: 0.100000\n",
      "2022-05-30 20:12:07,478 epoch 1 - iter 15/33 - loss 3.38192490 - samples/sec: 14.69 - lr: 0.100000\n",
      "2022-05-30 20:12:09,769 epoch 1 - iter 18/33 - loss 3.35312441 - samples/sec: 41.92 - lr: 0.100000\n",
      "2022-05-30 20:12:38,078 epoch 1 - iter 21/33 - loss 3.30142491 - samples/sec: 3.39 - lr: 0.100000\n",
      "2022-05-30 20:13:10,649 epoch 1 - iter 24/33 - loss 3.22898230 - samples/sec: 2.95 - lr: 0.100000\n",
      "2022-05-30 20:13:24,964 epoch 1 - iter 27/33 - loss 3.18944891 - samples/sec: 6.71 - lr: 0.100000\n",
      "2022-05-30 20:13:57,491 epoch 1 - iter 30/33 - loss 3.15049124 - samples/sec: 2.95 - lr: 0.100000\n",
      "2022-05-30 20:14:03,693 epoch 1 - iter 33/33 - loss 3.12502801 - samples/sec: 15.48 - lr: 0.100000\n",
      "2022-05-30 20:14:03,696 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:14:03,697 EPOCH 1 done: loss 3.1250 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:14:04,903 Evaluating as a multi-label problem: False\n",
      "2022-05-30 20:14:04,948 DEV : loss 2.724097490310669 - f1-score (micro avg)  0.1644\n",
      "2022-05-30 20:14:04,958 BAD EPOCHS (no improvement): 0\n",
      "2022-05-30 20:14:04,959 saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:14:06,044 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:14:21,719 epoch 2 - iter 3/33 - loss 2.72885470 - samples/sec: 6.13 - lr: 0.100000\n",
      "2022-05-30 20:14:28,931 epoch 2 - iter 6/33 - loss 2.70177667 - samples/sec: 13.31 - lr: 0.100000\n",
      "2022-05-30 20:14:42,041 epoch 2 - iter 9/33 - loss 2.69165108 - samples/sec: 7.32 - lr: 0.100000\n",
      "2022-05-30 20:14:52,969 epoch 2 - iter 12/33 - loss 2.68578417 - samples/sec: 8.79 - lr: 0.100000\n",
      "2022-05-30 20:15:26,832 epoch 2 - iter 15/33 - loss 2.64568502 - samples/sec: 2.84 - lr: 0.100000\n",
      "2022-05-30 20:15:39,120 epoch 2 - iter 18/33 - loss 2.62036188 - samples/sec: 7.82 - lr: 0.100000\n",
      "2022-05-30 20:16:10,473 epoch 2 - iter 21/33 - loss 2.58075715 - samples/sec: 3.06 - lr: 0.100000\n",
      "2022-05-30 20:16:30,081 epoch 2 - iter 24/33 - loss 2.54931108 - samples/sec: 4.90 - lr: 0.100000\n",
      "2022-05-30 20:16:41,437 epoch 2 - iter 27/33 - loss 2.52497775 - samples/sec: 8.46 - lr: 0.100000\n",
      "2022-05-30 20:16:53,725 epoch 2 - iter 30/33 - loss 2.49145644 - samples/sec: 7.81 - lr: 0.100000\n",
      "2022-05-30 20:17:03,060 epoch 2 - iter 33/33 - loss 2.47199431 - samples/sec: 10.29 - lr: 0.100000\n",
      "2022-05-30 20:17:03,061 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:17:03,063 EPOCH 2 done: loss 2.4720 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:17:04,347 Evaluating as a multi-label problem: False\n",
      "2022-05-30 20:17:04,390 DEV : loss 1.9358590841293335 - f1-score (micro avg)  0.417\n",
      "2022-05-30 20:17:04,400 BAD EPOCHS (no improvement): 0\n",
      "2022-05-30 20:17:04,403 saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:17:05,452 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:17:13,545 epoch 3 - iter 3/33 - loss 2.12275659 - samples/sec: 11.87 - lr: 0.100000\n",
      "2022-05-30 20:17:17,163 epoch 3 - iter 6/33 - loss 2.10434756 - samples/sec: 26.54 - lr: 0.100000\n",
      "2022-05-30 20:17:40,994 epoch 3 - iter 9/33 - loss 2.09913100 - samples/sec: 4.03 - lr: 0.100000\n",
      "2022-05-30 20:17:59,329 epoch 3 - iter 12/33 - loss 2.04959239 - samples/sec: 5.24 - lr: 0.100000\n",
      "2022-05-30 20:18:08,513 epoch 3 - iter 15/33 - loss 2.02260180 - samples/sec: 10.46 - lr: 0.100000\n",
      "2022-05-30 20:18:37,767 epoch 3 - iter 18/33 - loss 2.00542831 - samples/sec: 3.28 - lr: 0.100000\n",
      "2022-05-30 20:18:54,070 epoch 3 - iter 21/33 - loss 1.97620598 - samples/sec: 5.89 - lr: 0.100000\n",
      "2022-05-30 20:18:59,256 epoch 3 - iter 24/33 - loss 1.95787423 - samples/sec: 18.52 - lr: 0.100000\n",
      "2022-05-30 20:19:09,213 epoch 3 - iter 27/33 - loss 1.94037574 - samples/sec: 9.64 - lr: 0.100000\n",
      "2022-05-30 20:19:30,968 epoch 3 - iter 30/33 - loss 1.91822651 - samples/sec: 4.41 - lr: 0.100000\n",
      "2022-05-30 20:19:47,596 epoch 3 - iter 33/33 - loss 1.90095185 - samples/sec: 5.77 - lr: 0.100000\n",
      "2022-05-30 20:19:47,597 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:19:47,598 EPOCH 3 done: loss 1.9010 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:19:48,774 Evaluating as a multi-label problem: False\n",
      "2022-05-30 20:19:48,816 DEV : loss 1.4458906650543213 - f1-score (micro avg)  0.564\n",
      "2022-05-30 20:19:48,826 BAD EPOCHS (no improvement): 0\n",
      "2022-05-30 20:19:48,828 saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:19:49,737 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:19:56,377 epoch 4 - iter 3/33 - loss 1.65891652 - samples/sec: 14.46 - lr: 0.100000\n",
      "2022-05-30 20:20:27,325 epoch 4 - iter 6/33 - loss 1.67923468 - samples/sec: 3.10 - lr: 0.100000\n",
      "2022-05-30 20:20:44,840 epoch 4 - iter 9/33 - loss 1.65509067 - samples/sec: 5.48 - lr: 0.100000\n",
      "2022-05-30 20:20:59,843 epoch 4 - iter 12/33 - loss 1.66561404 - samples/sec: 6.40 - lr: 0.100000\n",
      "2022-05-30 20:21:03,588 epoch 4 - iter 15/33 - loss 1.65561167 - samples/sec: 25.64 - lr: 0.100000\n",
      "2022-05-30 20:21:13,668 epoch 4 - iter 18/33 - loss 1.64403494 - samples/sec: 9.53 - lr: 0.100000\n",
      "2022-05-30 20:21:27,575 epoch 4 - iter 21/33 - loss 1.63121708 - samples/sec: 6.90 - lr: 0.100000\n",
      "2022-05-30 20:21:43,250 epoch 4 - iter 24/33 - loss 1.62295161 - samples/sec: 6.12 - lr: 0.100000\n",
      "2022-05-30 20:22:01,440 epoch 4 - iter 27/33 - loss 1.61534880 - samples/sec: 5.28 - lr: 0.100000\n",
      "2022-05-30 20:22:15,199 epoch 4 - iter 30/33 - loss 1.60208944 - samples/sec: 6.98 - lr: 0.100000\n",
      "2022-05-30 20:22:40,399 epoch 4 - iter 33/33 - loss 1.58860554 - samples/sec: 3.81 - lr: 0.100000\n",
      "2022-05-30 20:22:40,400 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:22:40,401 EPOCH 4 done: loss 1.5886 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:22:42,707 Evaluating as a multi-label problem: False\n",
      "2022-05-30 20:22:42,750 DEV : loss 1.1749775409698486 - f1-score (micro avg)  0.6469\n",
      "2022-05-30 20:22:42,760 BAD EPOCHS (no improvement): 0\n",
      "2022-05-30 20:22:42,762 saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:22:43,704 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:22:58,221 epoch 5 - iter 3/33 - loss 1.55674657 - samples/sec: 6.61 - lr: 0.100000\n",
      "2022-05-30 20:23:04,523 epoch 5 - iter 6/33 - loss 1.47300834 - samples/sec: 15.24 - lr: 0.100000\n",
      "2022-05-30 20:23:13,923 epoch 5 - iter 9/33 - loss 1.47925597 - samples/sec: 10.21 - lr: 0.100000\n",
      "2022-05-30 20:23:24,762 epoch 5 - iter 12/33 - loss 1.46177484 - samples/sec: 8.86 - lr: 0.100000\n",
      "2022-05-30 20:23:30,545 epoch 5 - iter 15/33 - loss 1.45253537 - samples/sec: 16.61 - lr: 0.100000\n",
      "2022-05-30 20:23:41,065 epoch 5 - iter 18/33 - loss 1.45033684 - samples/sec: 9.13 - lr: 0.100000\n",
      "2022-05-30 20:23:55,224 epoch 5 - iter 21/33 - loss 1.45016184 - samples/sec: 6.78 - lr: 0.100000\n",
      "2022-05-30 20:24:15,961 epoch 5 - iter 24/33 - loss 1.44072145 - samples/sec: 4.63 - lr: 0.100000\n",
      "2022-05-30 20:24:55,044 epoch 5 - iter 27/33 - loss 1.43397964 - samples/sec: 2.46 - lr: 0.100000\n",
      "2022-05-30 20:25:05,592 epoch 5 - iter 30/33 - loss 1.42561300 - samples/sec: 9.10 - lr: 0.100000\n",
      "2022-05-30 20:25:11,501 epoch 5 - iter 33/33 - loss 1.41819789 - samples/sec: 16.25 - lr: 0.100000\n",
      "2022-05-30 20:25:11,503 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:25:11,503 EPOCH 5 done: loss 1.4182 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:25:12,579 Evaluating as a multi-label problem: False\n",
      "2022-05-30 20:25:12,636 DEV : loss 1.0181444883346558 - f1-score (micro avg)  0.6737\n",
      "2022-05-30 20:25:12,647 BAD EPOCHS (no improvement): 0\n",
      "2022-05-30 20:25:12,649 saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:25:13,433 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:25:36,178 epoch 6 - iter 3/33 - loss 1.35625019 - samples/sec: 4.22 - lr: 0.100000\n",
      "2022-05-30 20:25:57,664 epoch 6 - iter 6/33 - loss 1.35178838 - samples/sec: 4.47 - lr: 0.100000\n",
      "2022-05-30 20:26:10,182 epoch 6 - iter 9/33 - loss 1.33675522 - samples/sec: 7.67 - lr: 0.100000\n",
      "2022-05-30 20:26:20,152 epoch 6 - iter 12/33 - loss 1.33301144 - samples/sec: 9.63 - lr: 0.100000\n",
      "2022-05-30 20:26:46,667 epoch 6 - iter 15/33 - loss 1.33165848 - samples/sec: 3.62 - lr: 0.100000\n",
      "2022-05-30 20:26:54,268 epoch 6 - iter 18/33 - loss 1.32819350 - samples/sec: 12.63 - lr: 0.100000\n",
      "2022-05-30 20:27:04,184 epoch 6 - iter 21/33 - loss 1.31558992 - samples/sec: 9.68 - lr: 0.100000\n",
      "2022-05-30 20:27:15,298 epoch 6 - iter 24/33 - loss 1.30728272 - samples/sec: 8.64 - lr: 0.100000\n",
      "2022-05-30 20:27:41,076 epoch 6 - iter 27/33 - loss 1.30173201 - samples/sec: 3.72 - lr: 0.100000\n",
      "2022-05-30 20:27:49,427 epoch 6 - iter 30/33 - loss 1.29862568 - samples/sec: 11.50 - lr: 0.100000\n",
      "2022-05-30 20:28:04,718 epoch 6 - iter 33/33 - loss 1.30082295 - samples/sec: 6.28 - lr: 0.100000\n",
      "2022-05-30 20:28:04,719 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:28:04,720 EPOCH 6 done: loss 1.3008 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:28:06,067 Evaluating as a multi-label problem: False\n",
      "2022-05-30 20:28:06,107 DEV : loss 0.9527015686035156 - f1-score (micro avg)  0.6816\n",
      "2022-05-30 20:28:06,116 BAD EPOCHS (no improvement): 0\n",
      "2022-05-30 20:28:06,118 saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:28:07,047 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:28:17,611 epoch 7 - iter 3/33 - loss 1.23025922 - samples/sec: 9.09 - lr: 0.100000\n",
      "2022-05-30 20:28:48,721 epoch 7 - iter 6/33 - loss 1.23143581 - samples/sec: 3.09 - lr: 0.100000\n",
      "2022-05-30 20:29:11,532 epoch 7 - iter 9/33 - loss 1.22365532 - samples/sec: 4.21 - lr: 0.100000\n",
      "2022-05-30 20:29:20,675 epoch 7 - iter 12/33 - loss 1.21130657 - samples/sec: 10.50 - lr: 0.100000\n",
      "2022-05-30 20:29:28,717 epoch 7 - iter 15/33 - loss 1.19962493 - samples/sec: 11.94 - lr: 0.100000\n",
      "2022-05-30 20:29:53,074 epoch 7 - iter 18/33 - loss 1.20288219 - samples/sec: 3.94 - lr: 0.100000\n",
      "2022-05-30 20:30:07,831 epoch 7 - iter 21/33 - loss 1.21144868 - samples/sec: 6.51 - lr: 0.100000\n",
      "2022-05-30 20:30:13,951 epoch 7 - iter 24/33 - loss 1.20655801 - samples/sec: 15.69 - lr: 0.100000\n",
      "2022-05-30 20:30:23,867 epoch 7 - iter 27/33 - loss 1.20227806 - samples/sec: 9.68 - lr: 0.100000\n",
      "2022-05-30 20:30:35,526 epoch 7 - iter 30/33 - loss 1.20636603 - samples/sec: 8.23 - lr: 0.100000\n",
      "2022-05-30 20:30:47,604 epoch 7 - iter 33/33 - loss 1.21131915 - samples/sec: 7.95 - lr: 0.100000\n",
      "2022-05-30 20:30:47,606 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:30:47,606 EPOCH 7 done: loss 1.2113 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:30:48,678 Evaluating as a multi-label problem: False\n",
      "2022-05-30 20:30:48,716 DEV : loss 0.8962116241455078 - f1-score (micro avg)  0.7063\n",
      "2022-05-30 20:30:48,724 BAD EPOCHS (no improvement): 0\n",
      "2022-05-30 20:30:48,731 saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:30:49,708 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:31:01,902 epoch 8 - iter 3/33 - loss 1.21045459 - samples/sec: 7.87 - lr: 0.100000\n",
      "2022-05-30 20:31:09,325 epoch 8 - iter 6/33 - loss 1.20737316 - samples/sec: 12.94 - lr: 0.100000\n",
      "2022-05-30 20:31:18,150 epoch 8 - iter 9/33 - loss 1.18274787 - samples/sec: 10.88 - lr: 0.100000\n",
      "2022-05-30 20:31:49,179 epoch 8 - iter 12/33 - loss 1.18751066 - samples/sec: 3.09 - lr: 0.100000\n",
      "2022-05-30 20:32:09,758 epoch 8 - iter 15/33 - loss 1.16759019 - samples/sec: 4.67 - lr: 0.100000\n",
      "2022-05-30 20:32:25,007 epoch 8 - iter 18/33 - loss 1.17410156 - samples/sec: 6.30 - lr: 0.100000\n",
      "2022-05-30 20:32:38,459 epoch 8 - iter 21/33 - loss 1.15942075 - samples/sec: 7.14 - lr: 0.100000\n",
      "2022-05-30 20:32:54,451 epoch 8 - iter 24/33 - loss 1.16295099 - samples/sec: 6.00 - lr: 0.100000\n",
      "2022-05-30 20:33:02,398 epoch 8 - iter 27/33 - loss 1.15906481 - samples/sec: 12.08 - lr: 0.100000\n",
      "2022-05-30 20:33:12,655 epoch 8 - iter 30/33 - loss 1.15422132 - samples/sec: 9.36 - lr: 0.100000\n",
      "2022-05-30 20:33:27,900 epoch 8 - iter 33/33 - loss 1.15356702 - samples/sec: 6.30 - lr: 0.100000\n",
      "2022-05-30 20:33:27,901 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:33:27,901 EPOCH 8 done: loss 1.1536 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:33:28,995 Evaluating as a multi-label problem: False\n",
      "2022-05-30 20:33:29,036 DEV : loss 0.8353503346443176 - f1-score (micro avg)  0.7356\n",
      "2022-05-30 20:33:29,045 BAD EPOCHS (no improvement): 0\n",
      "2022-05-30 20:33:29,048 saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:33:29,765 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:33:55,924 epoch 9 - iter 3/33 - loss 1.11320682 - samples/sec: 3.67 - lr: 0.100000\n",
      "2022-05-30 20:34:04,225 epoch 9 - iter 6/33 - loss 1.12227842 - samples/sec: 11.57 - lr: 0.100000\n",
      "2022-05-30 20:34:14,114 epoch 9 - iter 9/33 - loss 1.12338716 - samples/sec: 9.71 - lr: 0.100000\n",
      "2022-05-30 20:34:29,134 epoch 9 - iter 12/33 - loss 1.10671165 - samples/sec: 6.39 - lr: 0.100000\n",
      "2022-05-30 20:34:47,398 epoch 9 - iter 15/33 - loss 1.10297535 - samples/sec: 5.26 - lr: 0.100000\n",
      "2022-05-30 20:35:05,370 epoch 9 - iter 18/33 - loss 1.11597242 - samples/sec: 5.34 - lr: 0.100000\n",
      "2022-05-30 20:35:16,669 epoch 9 - iter 21/33 - loss 1.10733720 - samples/sec: 8.50 - lr: 0.100000\n",
      "2022-05-30 20:35:33,130 epoch 9 - iter 24/33 - loss 1.10741175 - samples/sec: 5.83 - lr: 0.100000\n",
      "2022-05-30 20:35:50,476 epoch 9 - iter 27/33 - loss 1.09937668 - samples/sec: 5.54 - lr: 0.100000\n",
      "2022-05-30 20:36:00,959 epoch 9 - iter 30/33 - loss 1.10029243 - samples/sec: 9.16 - lr: 0.100000\n",
      "2022-05-30 20:36:10,125 epoch 9 - iter 33/33 - loss 1.09927119 - samples/sec: 10.48 - lr: 0.100000\n",
      "2022-05-30 20:36:10,127 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:36:10,127 EPOCH 9 done: loss 1.0993 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:36:11,264 Evaluating as a multi-label problem: False\n",
      "2022-05-30 20:36:11,304 DEV : loss 0.7743029594421387 - f1-score (micro avg)  0.7593\n",
      "2022-05-30 20:36:11,314 BAD EPOCHS (no improvement): 0\n",
      "2022-05-30 20:36:11,316 saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:36:12,328 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:36:36,242 epoch 10 - iter 3/33 - loss 1.10964287 - samples/sec: 4.01 - lr: 0.100000\n",
      "2022-05-30 20:36:51,366 epoch 10 - iter 6/33 - loss 1.08613679 - samples/sec: 6.35 - lr: 0.100000\n",
      "2022-05-30 20:36:59,953 epoch 10 - iter 9/33 - loss 1.06080668 - samples/sec: 11.18 - lr: 0.100000\n",
      "2022-05-30 20:37:14,108 epoch 10 - iter 12/33 - loss 1.05732296 - samples/sec: 6.78 - lr: 0.100000\n",
      "2022-05-30 20:37:25,781 epoch 10 - iter 15/33 - loss 1.05057511 - samples/sec: 8.23 - lr: 0.100000\n",
      "2022-05-30 20:37:48,622 epoch 10 - iter 18/33 - loss 1.05391322 - samples/sec: 4.20 - lr: 0.100000\n",
      "2022-05-30 20:37:57,049 epoch 10 - iter 21/33 - loss 1.05322709 - samples/sec: 11.39 - lr: 0.100000\n",
      "2022-05-30 20:38:07,491 epoch 10 - iter 24/33 - loss 1.05166623 - samples/sec: 9.20 - lr: 0.100000\n",
      "2022-05-30 20:38:23,944 epoch 10 - iter 27/33 - loss 1.05757683 - samples/sec: 5.84 - lr: 0.100000\n",
      "2022-05-30 20:38:31,411 epoch 10 - iter 30/33 - loss 1.05346399 - samples/sec: 12.86 - lr: 0.100000\n",
      "2022-05-30 20:38:40,722 epoch 10 - iter 33/33 - loss 1.04809684 - samples/sec: 10.31 - lr: 0.100000\n",
      "2022-05-30 20:38:40,723 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:38:40,724 EPOCH 10 done: loss 1.0481 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:38:43,019 Evaluating as a multi-label problem: False\n",
      "2022-05-30 20:38:43,057 DEV : loss 0.7261837124824524 - f1-score (micro avg)  0.7555\n",
      "2022-05-30 20:38:43,066 BAD EPOCHS (no improvement): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:38:44,283 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-30 20:38:44,290 loading file resources/taggers/example-upos/best-model.pt\n",
      "2022-05-30 20:38:45,330 SequenceTagger predicts: Dictionary with 44 tags: <unk>, NNP, NN, IN, DT, CC, NNS, ,, JJ, CD, :, ., VB, VBN, TO, VBZ, RB, MD, VBG, VBD, VBP, PRP$, POS, WDT, LS, PRP, NNPS, '', JJR, WRB, JJS, $, ``, -NONE-, PDT, EX, RBR, RP, WP$, -X-, WP, RBS, <START>, <STOP>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:38:49,725 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-30 20:38:49,819 0.7528\t0.7528\t0.7528\t0.7528\n",
      "2022-05-30 20:38:49,820 \n",
      "Results:\n",
      "- F-score (micro) 0.7528\n",
      "- F-score (macro) 0.4185\n",
      "- Accuracy 0.7528\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NN     0.4982    0.7846    0.6095      1806\n",
      "          IN     0.8839    0.9178    0.9005      1593\n",
      "          DT     0.9374    0.9828    0.9596      1570\n",
      "         NNP     0.8002    0.4554    0.5804      1961\n",
      "          CC     0.9650    0.9822    0.9735       673\n",
      "          CD     0.7493    0.8813    0.8099       573\n",
      "           ,     0.9806    0.9712    0.9759       626\n",
      "           :     0.6561    0.8914    0.7559       488\n",
      "          JJ     0.7052    0.4513    0.5503       636\n",
      "         NNS     0.5881    0.4860    0.5322       570\n",
      "          VB     0.7111    0.8184    0.7610       391\n",
      "           .     0.9833    0.9976    0.9904       413\n",
      "          TO     0.9732    1.0000    0.9864       363\n",
      "         VBN     0.5864    0.6142    0.6000       337\n",
      "          MD     0.8672    0.9867    0.9231       225\n",
      "          RB     0.2947    0.3035    0.2990       201\n",
      "         VBZ     0.7770    0.7347    0.7552       147\n",
      "         VBG     0.5667    0.1405    0.2252       121\n",
      "         VBP     0.6957    0.3299    0.4476        97\n",
      "         VBD     0.2083    0.0427    0.0709       117\n",
      "        PRP$     1.0000    0.7846    0.8793        65\n",
      "         WDT     0.7647    0.4727    0.5843        55\n",
      "         PRP     0.7714    0.5294    0.6279        51\n",
      "         POS     0.6957    1.0000    0.8205        32\n",
      "        NNPS     0.0000    0.0000    0.0000        41\n",
      "          LS     0.5000    0.0690    0.1212        29\n",
      "         JJR     0.0000    0.0000    0.0000        16\n",
      "          ''     0.0000    0.0000    0.0000        10\n",
      "          RP     0.0000    0.0000    0.0000         9\n",
      "         WRB     0.0000    0.0000    0.0000         6\n",
      "          ``     0.0000    0.0000    0.0000         6\n",
      "      -NONE-     0.0000    0.0000    0.0000         5\n",
      "         WP$     0.0000    0.0000    0.0000         3\n",
      "         JJS     0.0000    0.0000    0.0000         2\n",
      "          EX     0.0000    0.0000    0.0000         2\n",
      "         -X-     0.0000    0.0000    0.0000         2\n",
      "         RBR     0.0000    0.0000    0.0000         2\n",
      "           $     0.0000    0.0000    0.0000         2\n",
      "         PDT     0.0000    0.0000    0.0000         1\n",
      "          WP     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.7528     13248\n",
      "   macro avg     0.4540    0.4157    0.4185     13248\n",
      "weighted avg     0.7618    0.7528    0.7403     13248\n",
      "\n",
      "2022-05-30 20:38:49,820 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'test_score': 0.7527928743961353,\n 'dev_score_history': [0.16437118988484986,\n  0.41702415895235945,\n  0.5640099345224656,\n  0.646872883269361,\n  0.6737412508466922,\n  0.6816437118988485,\n  0.7062542334612779,\n  0.7356062316550012,\n  0.7593136148114699,\n  0.7554752765861368],\n 'train_loss_history': [3.125028008861172,\n  2.4719943058015272,\n  1.9009518467574036,\n  1.588605540085329,\n  1.4181978933865742,\n  1.300822951266437,\n  1.2113191520194306,\n  1.153567022745291,\n  1.099271186272349,\n  1.0480968444368695],\n 'dev_loss_history': [2.724097490310669,\n  1.9358590841293335,\n  1.4458906650543213,\n  1.1749775409698486,\n  1.0181444883346558,\n  0.9527015686035156,\n  0.8962116241455078,\n  0.8353503346443176,\n  0.7743029594421387,\n  0.7261837124824524]}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-upos',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "df = df_from_filings()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "          cik report_type       report_identity                 file  \\\n0  0001553197       N-CSR  0000051931-14-000113  full-submission.txt   \n1  0001553197       N-CSR  0000051931-19-000118  full-submission.txt   \n2  0001553197       N-CSR  0000051931-17-000292  full-submission.txt   \n3  0001553197       N-CSR  0000925950-20-000008  full-submission.txt   \n4  0001553197       N-CSR  0000051931-15-000081  full-submission.txt   \n5  0001553197      N-CSRS  0000051931-14-000608  full-submission.txt   \n6  0001553197      N-CSRS  0000925950-20-000123  full-submission.txt   \n7  0001553197      N-CSRS  0000051931-16-002670  full-submission.txt   \n8  0001553197      N-CSRS  0000051931-15-000606  full-submission.txt   \n9  0001553197      N-CSRS  0000051931-18-000713  full-submission.txt   \n\n                                                text  \n0  0000051931-14-000113.txt : 20140131 0000051931...  \n1  0000051931-19-000118.txt : 20190131 0000051931...  \n2  0000051931-17-000292.txt : 20170131 0000051931...  \n3  0000925950-20-000008.txt : 20200131 0000925950...  \n4  0000051931-15-000081.txt : 20150130 0000051931...  \n5  0000051931-14-000608.txt : 20140731 0000051931...  \n6  0000925950-20-000123.txt : 20200731 0000925950...  \n7  0000051931-16-002670.txt : 20160729 0000051931...  \n8  0000051931-15-000606.txt : 20150731 0000051931...  \n9  0000051931-18-000713.txt : 20180731 0000051931...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cik</th>\n      <th>report_type</th>\n      <th>report_identity</th>\n      <th>file</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0001553197</td>\n      <td>N-CSR</td>\n      <td>0000051931-14-000113</td>\n      <td>full-submission.txt</td>\n      <td>0000051931-14-000113.txt : 20140131 0000051931...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0001553197</td>\n      <td>N-CSR</td>\n      <td>0000051931-19-000118</td>\n      <td>full-submission.txt</td>\n      <td>0000051931-19-000118.txt : 20190131 0000051931...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0001553197</td>\n      <td>N-CSR</td>\n      <td>0000051931-17-000292</td>\n      <td>full-submission.txt</td>\n      <td>0000051931-17-000292.txt : 20170131 0000051931...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001553197</td>\n      <td>N-CSR</td>\n      <td>0000925950-20-000008</td>\n      <td>full-submission.txt</td>\n      <td>0000925950-20-000008.txt : 20200131 0000925950...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001553197</td>\n      <td>N-CSR</td>\n      <td>0000051931-15-000081</td>\n      <td>full-submission.txt</td>\n      <td>0000051931-15-000081.txt : 20150130 0000051931...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0001553197</td>\n      <td>N-CSRS</td>\n      <td>0000051931-14-000608</td>\n      <td>full-submission.txt</td>\n      <td>0000051931-14-000608.txt : 20140731 0000051931...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0001553197</td>\n      <td>N-CSRS</td>\n      <td>0000925950-20-000123</td>\n      <td>full-submission.txt</td>\n      <td>0000925950-20-000123.txt : 20200731 0000925950...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0001553197</td>\n      <td>N-CSRS</td>\n      <td>0000051931-16-002670</td>\n      <td>full-submission.txt</td>\n      <td>0000051931-16-002670.txt : 20160729 0000051931...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0001553197</td>\n      <td>N-CSRS</td>\n      <td>0000051931-15-000606</td>\n      <td>full-submission.txt</td>\n      <td>0000051931-15-000606.txt : 20150731 0000051931...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0001553197</td>\n      <td>N-CSRS</td>\n      <td>0000051931-18-000713</td>\n      <td>full-submission.txt</td>\n      <td>0000051931-18-000713.txt : 20180731 0000051931...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-31 20:34:12,230 loading file resources/taggers/example-upos/final-model.pt\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'document_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-48-9d61d715a208>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mflair\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSentence\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# load the model you trained\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextClassifier\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'resources/taggers/example-upos/final-model.pt'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/flair/nn/model.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(cls, model_path)\u001B[0m\n\u001B[1;32m    142\u001B[0m             \u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmap_location\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"cpu\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 144\u001B[0;31m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_init_model_with_state_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;34m\"model_card\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mstate\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/flair/models/text_classification_model.py\u001B[0m in \u001B[0;36m_init_model_with_state_dict\u001B[0;34m(cls, state, **kwargs)\u001B[0m\n\u001B[1;32m     98\u001B[0m         return super()._init_model_with_state_dict(\n\u001B[1;32m     99\u001B[0m             \u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 100\u001B[0;31m             \u001B[0mdocument_embeddings\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"document_embeddings\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    101\u001B[0m             \u001B[0mlabel_dictionary\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"label_dictionary\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m             \u001B[0mlabel_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlabel_type\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'document_embeddings'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from flair.data import Sentence\n",
    "# load the model you trained\n",
    "model = TextClassifier.load('resources/taggers/example-upos/final-model.pt')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zad/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LS 0.5226038098335266\n",
      "NNP 0.452282577753067\n",
      "NN 0.592628002166748\n",
      "NN 0.5850546956062317\n",
      "PRP 0.5451674461364746\n",
      "NN 0.5405938029289246\n",
      "NN 0.5732719898223877\n",
      "PRP 0.5248014330863953\n",
      "PRP 0.5237467288970947\n",
      "NN 0.602861762046814\n",
      "PRP 0.523640513420105\n",
      "NNP 0.44740673899650574\n",
      "NNP 0.4598245620727539\n",
      "NN 0.45448875427246094\n",
      "PRP 0.5236288905143738\n",
      "NN 0.5979695320129395\n",
      "DT 0.9386390447616577\n",
      "NN 0.5036590695381165\n",
      "NNP 0.5517086386680603\n",
      "NNP 0.41828852891921997\n",
      "NNP 0.4273374676704407\n",
      "NN 0.5882295370101929\n",
      "NN 0.42913687229156494\n",
      "PRP 0.5331946015357971\n",
      "NN 0.42279863357543945\n",
      "NN 0.5979107022285461\n",
      "PRP 0.5258358716964722\n",
      "NN 0.5056564807891846\n",
      "NN 0.4682164490222931\n",
      "DT 0.9423786401748657\n",
      "NN 0.6137647032737732\n",
      "PRP 0.49864640831947327\n",
      "NNP 0.43346765637397766\n",
      "PRP 0.5430770516395569\n",
      "NNP 0.43728509545326233\n",
      "NN 0.4322200119495392\n",
      "NN 0.4860096871852875\n",
      "NN 0.6079819798469543\n",
      "DT 0.9345417022705078\n",
      "NN 0.4322192966938019\n",
      "NNP 0.4234294295310974\n",
      "NN 0.5196259021759033\n",
      "NN 0.39640605449676514\n",
      "NNP 0.4187186658382416\n",
      "DT 0.948324978351593\n",
      "NN 0.5647856593132019\n",
      "NNP 0.4135495722293854\n",
      "NN 0.6481320261955261\n",
      "NNP 0.45148420333862305\n",
      "NN 0.6126755475997925\n",
      "NNP 0.39035382866859436\n",
      "NN 0.5088720917701721\n",
      "NNP 0.6162674427032471\n",
      "NN 0.559476912021637\n",
      "DT 0.9345417022705078\n",
      "NN 0.5850440263748169\n",
      "CD 0.5536673665046692\n",
      "CD 0.6271578669548035\n",
      "NNP 0.42590466141700745\n",
      "CD 0.5937138795852661\n",
      "CD 0.6230281591415405\n",
      "CD 0.6532461643218994\n",
      "NN 0.6841249465942383\n",
      "NN 0.4322449862957001\n",
      "PRP 0.523640513420105\n",
      "NNP 0.44740673899650574\n",
      "NNP 0.4598245620727539\n",
      "NN 0.4544888138771057\n",
      "NN 0.5979061126708984\n",
      "DT 0.9386390447616577\n",
      "NN 0.5036590695381165\n",
      "NNP 0.5517086386680603\n",
      "DT 0.9345417022705078\n",
      "PRP 0.5427764058113098\n",
      "NNP 0.44726285338401794\n",
      "NNP 0.35427725315093994\n",
      "NN 0.5961350798606873\n",
      "NN 0.44815245270729065\n",
      "NN 0.5227053761482239\n",
      "NN 0.448152631521225\n",
      "NN 0.6492502093315125\n",
      "CD 0.6517797708511353\n",
      "NN 0.4310825765132904\n",
      "CD 0.6899343729019165\n",
      "NN 0.4585062861442566\n",
      "NN 0.4310820400714874\n",
      "CD 0.6425681710243225\n",
      "CD 0.6528735756874084\n",
      "NN 0.4146762192249298\n",
      "NNP 0.6465380191802979\n",
      "NNP 0.5470125079154968\n",
      "CD 0.6516740322113037\n",
      "CD 0.6858533024787903\n",
      "NNP 0.5470125079154968\n",
      "NNP 0.547012448310852\n",
      "NN 0.594917893409729\n",
      "NN 0.553631603717804\n",
      "NN 0.581345796585083\n",
      "NNP 0.42056459188461304\n",
      "NNP 0.41841623187065125\n",
      "NN 0.4188079237937927\n",
      "NN 0.43630319833755493\n",
      "NNP 0.4330786466598511\n",
      "NNP 0.4180922508239746\n",
      "NN 0.5131296515464783\n",
      "NN 0.6740504503250122\n",
      "NN 0.5719956159591675\n",
      "NNP 0.5865075588226318\n",
      "CD 0.46891483664512634\n",
      "NNP 0.4690339267253876\n",
      "NNP 0.42160406708717346\n",
      "DT 0.9340707063674927\n",
      "NNP 0.4183698296546936\n",
      "NNP 0.5510468482971191\n",
      "NN 0.6632978916168213\n",
      "PRP 0.5492669939994812\n",
      "PRP 0.5201753377914429\n",
      "NN 0.47093600034713745\n",
      "NN 0.5865516066551208\n",
      "NN 0.5833806991577148\n",
      "DT 0.9345417022705078\n",
      "NN 0.5368961095809937\n",
      "NN 0.6439853310585022\n",
      "NNP 0.41823941469192505\n",
      "NNP 0.4347672164440155\n",
      "CD 0.5157138109207153\n",
      "NN 0.4932202994823456\n",
      "NNP 0.4243198335170746\n",
      "NN 0.44368240237236023\n",
      "NNP 0.5141161680221558\n",
      "NNP 0.5456736087799072\n",
      "NN 0.48366448283195496\n",
      "NN 0.4404994547367096\n",
      "NN 0.5028715133666992\n",
      "NNP 0.5456802248954773\n",
      "NN 0.5064507722854614\n",
      "NN 0.42922040820121765\n",
      "NN 0.5902587175369263\n",
      "NNP 0.5540876984596252\n",
      "NNP 0.44740650057792664\n",
      "NNP 0.6567325592041016\n",
      "DT 0.9345417022705078\n",
      "NNP 0.44740617275238037\n",
      "NNP 0.44075366854667664\n",
      "NNP 0.44075360894203186\n",
      "NNP 0.41821321845054626\n",
      "NNP 0.4187188744544983\n",
      "NN 0.6962091326713562\n",
      "NN 0.5968839526176453\n",
      "NN 0.5929965972900391\n",
      "NN 0.5929576754570007\n",
      "NN 0.43113741278648376\n",
      "NN 0.5983749628067017\n",
      "DT 0.9365270733833313\n",
      "CD 0.5428130626678467\n",
      "NNP 0.5833209753036499\n",
      "NN 0.46836575865745544\n",
      "PRP 0.524886965751648\n",
      "NN 0.5576789379119873\n",
      "PRP 0.5595962405204773\n",
      "NN 0.5847414135932922\n",
      "NN 0.5546242594718933\n",
      "NN 0.6107989549636841\n",
      "NN 0.5043208599090576\n",
      "NNP 0.41871878504753113\n",
      "PRP 0.524894118309021\n",
      "DT 0.9588728547096252\n",
      "PRP 0.5331946015357971\n",
      "NN 0.42198267579078674\n",
      "PRP 0.524894118309021\n",
      "PRP 0.518503725528717\n",
      "DT 0.9345417022705078\n",
      "DT 0.9292635321617126\n",
      "NNP 0.5543285012245178\n",
      "PRP 0.5248820185661316\n",
      "PRP 0.5214952826499939\n",
      "PRP 0.5428284406661987\n",
      "NNP 0.43645504117012024\n",
      "PRP 0.5248180031776428\n",
      "NNP 0.41956356167793274\n",
      "NNP 0.5540474653244019\n",
      "NNP 0.44423407316207886\n",
      "NN 0.47909387946128845\n",
      "NN 0.5022386908531189\n",
      "NNP 0.45472148060798645\n",
      "CD 0.5422607660293579\n",
      "NN 0.6157850027084351\n",
      "NNP 0.4837735891342163\n",
      "NN 0.6152633428573608\n",
      "NN 0.663303792476654\n",
      "PRP 0.5214923620223999\n",
      "NNP 0.43987563252449036\n",
      "NNP 0.41871902346611023\n",
      "NNP 0.5813479423522949\n",
      "NNP 0.48377466201782227\n",
      "NN 0.45168235898017883\n",
      "NN 0.6152633428573608\n",
      "NNP 0.4192918539047241\n",
      "NNP 0.4186983108520508\n",
      "NN 0.6211046576499939\n",
      "DT 0.9345417022705078\n",
      "NN 0.5893449187278748\n",
      "NNP 0.4209732711315155\n",
      "NN 0.614894688129425\n",
      "CD 0.5407547354698181\n",
      "NN 0.5090482831001282\n",
      "NNP 0.42054563760757446\n",
      "NN 0.4147096872329712\n",
      "DT 0.9575381875038147\n",
      "NNP 0.41871169209480286\n",
      "NN 0.6341188549995422\n",
      "NNP 0.42054569721221924\n",
      "NN 0.5627979040145874\n",
      "NNP 0.6224373579025269\n",
      "NNP 0.42392751574516296\n",
      "NNP 0.4373658001422882\n",
      "NN 0.4192142188549042\n",
      "NN 0.6341008543968201\n",
      "DT 0.9346915483474731\n",
      "NNP 0.4192647337913513\n",
      "NN 0.6043095588684082\n",
      "CD 0.5639162063598633\n",
      "NN 0.6439733505249023\n",
      "NN 0.6158318519592285\n",
      "PRP 0.5239893198013306\n",
      "NNP 0.41613662242889404\n",
      "NN 0.5959619879722595\n",
      "NN 0.5749773383140564\n",
      "NNP 0.45982229709625244\n",
      "NN 0.5959619879722595\n",
      "NN 0.5833806991577148\n",
      "NNP 0.4348042905330658\n",
      "NN 0.4888644516468048\n",
      "NN 0.42460837960243225\n",
      "PRP 0.5485953688621521\n",
      "DT 0.9505072832107544\n",
      "NNP 0.43308666348457336\n",
      "NN 0.5988234877586365\n",
      "NNP 0.41977086663246155\n",
      "DT 0.9561214447021484\n",
      "NN 0.5720404386520386\n",
      "NNP 0.4200073778629303\n",
      "PRP 0.5595358610153198\n",
      "DT 0.9365752935409546\n",
      "NNP 0.42000865936279297\n",
      "NN 0.48931169509887695\n",
      "CD 0.5034782290458679\n",
      "NN 0.42645263671875\n",
      "NNP 0.4176959991455078\n",
      "NNP 0.4215466380119324\n",
      "NN 0.42737582325935364\n",
      "NN 0.5959619879722595\n",
      "NNP 0.3709997534751892\n",
      "NN 0.4463633894920349\n",
      "NN 0.4139387607574463\n",
      "NNP 0.425057053565979\n",
      "NN 0.4464251399040222\n",
      "DT 0.9341652989387512\n",
      "NN 0.6152763366699219\n",
      "CD 0.5495291948318481\n",
      "NN 0.6346886157989502\n",
      "NNP 0.3709997534751892\n",
      "DT 0.9346915483474731\n",
      "PRP 0.5237282514572144\n",
      "NN 0.574975848197937\n",
      "NNP 0.42005476355552673\n",
      "NN 0.5949002504348755\n",
      "NNP 0.43139544129371643\n",
      "NNP 0.4172711968421936\n",
      "LS 0.5094321370124817\n",
      "NN 0.5106489658355713\n",
      "NNP 0.41793307662010193\n",
      "DT 0.9365270733833313\n",
      "DT 0.9295164346694946\n",
      "NN 0.5886202454566956\n",
      "DT 0.9346915483474731\n",
      "CD 0.5768033266067505\n",
      "CD 0.6195476055145264\n",
      "NN 0.6343008279800415\n",
      "CD 0.7108191251754761\n",
      "PRP 0.5223603248596191\n",
      "CD 0.6491593718528748\n",
      "CD 0.6850184798240662\n",
      "CD 0.723281979560852\n",
      "CD 0.6598764657974243\n",
      "CD 0.7070501446723938\n",
      "CD 0.5060104727745056\n",
      "NNP 0.5469307899475098\n",
      "NNP 0.4232262670993805\n",
      "NNP 0.5415700078010559\n",
      "NN 0.5903979539871216\n",
      "NNP 0.40661948919296265\n",
      "DT 0.951962947845459\n",
      "NNP 0.611571192741394\n",
      "NN 0.6813951134681702\n",
      "NNP 0.4559057950973511\n",
      "NN 0.42882251739501953\n",
      "DT 0.9338222146034241\n",
      "NN 0.5432147979736328\n",
      "NNP 0.4579782485961914\n",
      "NN 0.5281878709793091\n",
      "NNP 0.42135336995124817\n",
      "NNP 0.4266655445098877\n",
      "NN 0.5552681088447571\n",
      "PRP 0.5595197677612305\n",
      "NN 0.5460641384124756\n",
      "PRP 0.5595171451568604\n",
      "NN 0.5592084527015686\n",
      "NN 0.4145112633705139\n",
      "PRP 0.5595338940620422\n",
      "NNP 0.6363711953163147\n",
      "NNP 0.3562135398387909\n",
      "NNP 0.44631427526474\n",
      "DT 0.9346915483474731\n",
      "NNP 0.5019720196723938\n",
      "NN 0.5187656283378601\n",
      "NNP 0.5556990504264832\n",
      "NN 0.4510176479816437\n",
      "DT 0.9346915483474731\n",
      "NN 0.5810447931289673\n",
      "NN 0.47177189588546753\n",
      "NN 0.5714558362960815\n",
      "NN 0.6158816814422607\n",
      "NN 0.4656018912792206\n",
      "CD 0.5787031054496765\n",
      "CD 0.6214556694030762\n",
      "CD 0.665550172328949\n",
      "CD 0.7065322399139404\n",
      "CD 0.6420886516571045\n",
      "DT 0.9345417022705078\n",
      "NN 0.6770938634872437\n",
      "NN 0.5829735994338989\n",
      "NN 0.5846800208091736\n",
      "NNP 0.4647507965564728\n",
      "NN 0.5440483689308167\n",
      "NN 0.6808509826660156\n",
      "NNP 0.41941022872924805\n",
      "NNP 0.4237934350967407\n",
      "DT 0.9345417022705078\n",
      "NNP 0.46172648668289185\n",
      "NN 0.5440904498100281\n",
      "DT 0.9472761750221252\n",
      "NNP 0.4547692537307739\n",
      "PRP 0.5082356333732605\n",
      "DT 0.9345417022705078\n",
      "NNP 0.5436502695083618\n",
      "DT 0.9284737706184387\n",
      "NN 0.5375790596008301\n",
      "CD 0.5541082620620728\n",
      "NN 0.6699694395065308\n",
      "DT 0.9479500651359558\n",
      "NNP 0.513727605342865\n",
      "NNP 0.5509019494056702\n",
      "CD 0.574504017829895\n",
      "CD 0.5855464339256287\n",
      "CD 0.6147980093955994\n",
      "NNP 0.6884825825691223\n",
      "CD 0.663362979888916\n",
      "NN 0.59767085313797\n",
      "NN 0.6359685063362122\n",
      "NN 0.3982689678668976\n",
      "NNP 0.4306292235851288\n",
      "NN 0.5155015587806702\n",
      "PRP 0.5350980758666992\n",
      "NNP 0.45687609910964966\n",
      "NN 0.4229110777378082\n",
      "PRP 0.5340306162834167\n",
      "NN 0.6624804735183716\n",
      "NN 0.6624802947044373\n",
      "DT 0.9521328806877136\n",
      "NN 0.4246576428413391\n",
      "NNP 0.43037939071655273\n",
      "NNP 0.430360347032547\n",
      "DT 0.9298110008239746\n",
      "NN 0.4246576428413391\n",
      "NN 0.42465493083000183\n",
      "DT 0.9295288324356079\n",
      "NN 0.4240553081035614\n",
      "PRP 0.5349444150924683\n",
      "PRP 0.534017026424408\n",
      "PRP 0.534000039100647\n",
      "PRP 0.5338733196258545\n",
      "PRP 0.5341159105300903\n",
      "PRP 0.5329713225364685\n",
      "NN 0.4230092167854309\n",
      "NNP 0.4306301474571228\n",
      "NNP 0.4303482174873352\n",
      "NNP 0.5779271721839905\n",
      "NNP 0.5164836645126343\n",
      "PRP 0.5330549478530884\n",
      "NN 0.49533891677856445\n",
      "PRP 0.5328832864761353\n",
      "NN 0.5725613236427307\n",
      "NNP 0.4709489047527313\n",
      "DT 0.9345417022705078\n",
      "NN 0.590440571308136\n",
      "NN 0.4970974624156952\n",
      "NN 0.46043282747268677\n",
      "NNP 0.35089951753616333\n",
      "NNP 0.3508996367454529\n",
      "NN 0.4768850803375244\n",
      "NN 0.48423928022384644\n",
      "NN 0.4694744944572449\n",
      "NN 0.593716561794281\n",
      "PRP 0.5584860444068909\n",
      "NN 0.6541399359703064\n",
      "NN 0.6541395783424377\n",
      "NNP 0.43062907457351685\n",
      "NNP 0.43062907457351685\n",
      "NN 0.6774258613586426\n",
      "PRP 0.5584860444068909\n",
      "NN 0.6541399359703064\n",
      "NN 0.6541395783424377\n",
      "NNP 0.43062907457351685\n",
      "NNP 0.43062907457351685\n",
      "NN 0.6774258613586426\n",
      "NNP 0.5628798604011536\n",
      "NN 0.6000223159790039\n",
      "NN 0.6872175931930542\n",
      "NNP 0.45713552832603455\n",
      "NNP 0.562679648399353\n",
      "NN 0.5694998502731323\n"
     ]
    }
   ],
   "source": [
    "# create example sentence\n",
    "for sentence in TextBlob(df.text[0]).sentences:\n",
    "    s = Sentence(sentence)\n",
    "    # predict tags and print\n",
    "    model.predict(s)\n",
    "\n",
    "\n",
    "    print(s.tag, s.score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sentence' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-45-f33580314f09>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msentence\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscore\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Sentence' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "sentence.score\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "Sentence(\"0000051931-14-000113.txt : 20140131 0000051931-14-000113.hdr.sgml : 20140131 20140131142327ACCESSION NUMBER:0000051931-14-000113CONFORMED SUBMISSION TYPE:N-CSRPUBLIC DOCUMENT COUNT:12CONFORMED PERIOD OF REPORT:20131130FILED AS OF DATE:20140131DATE AS OF CHANGE:20140131EFFECTIVENESS DATE:20140131FILER:COMPANY DATA:COMPANY CONFORMED NAME:AMERICAN FUNDS INFLATION LINKED BOND FUNDCENTRAL INDEX KEY:0001553197IRS NUMBER:000000000STATE OF INCORPORATION:DEFILING VALUES:FORM TYPE:N-CSRSEC ACT:1940 ActSEC FILE NUMBER:811-22746FILM NUMBER:14564185BUSINESS ADDRESS:STREET 1:333 SOUTH HOPE STREET, 55TH FLOORCITY:LOS ANGELESSTATE:CAZIP:90071BUSINESS PHONE:213-486-9200MAIL ADDRESS:STREET 1:333 SOUTH HOPE STREET, 55TH FLOORCITY:LOS ANGELESSTATE:CAZIP:90071 0001553197 S000039115 AMERICAN FUNDS INFLATION LINKED BOND FUND C000120322 Class A C000120323 Class 529-F-1 C000120324 Class R-1 C000120325 Class R-2 C000120326 Class R-3 C000120327 Class R-4 C000120328 Class R-5 C000120329 Class R-6 C000120330 Class B C000120331 Class C C000120332 Class F-1 C000120333 Class F-2 C000120334 Class 529-A C000120335 Class 529-B C000120336 Class 529-C C000120337 Class 529-E N-CSR 1 ilbf_ncsr.htm N-CSR UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington, D.C. 20549 FORM N-CSR Certified Shareholder Report of Registered Management Investment Companies Investment Company Act File Number: 811-22746 American Funds Inflation Linked Bond Fund (Exact Name of Registrant as Specified in Charter) 6455 Irvine Center Drive Irvine, California 92618 (Address of Principal Executive Offices) Registrant's telephone number, including areacode: (949) 975-5000 Date of fiscal year end: November 30 Date of reporting period: November 30, 2013 Courtney R. Taylor American Funds Inflation Linked Bond Fund 6455 Irvine Center Drive Irvine, California 92618 (Name and Address of Agent for Service) Copies to: Michael Glazer Bingham McCutchen LLP 355 South Grand Avenue, Suite 4400 Los Angeles, California 90071 (Counsel for the Registrant) ITEM 1 – Reports to Stockholders American        Funds Inflation Linked Bond Fund SM Annual        report for the period ended November 30, 2013 American Funds Inflation Linked Bond Fund seeks to provide inflationprotection and income consistent with investment in inflation linked securities.\")"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(df.text[0]).sentences[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
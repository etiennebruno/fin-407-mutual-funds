{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reuters.fileids()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reuters.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reuters.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## categories overlap on the Reuters corpus\n",
    "print(reuters.categories('training/9865'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1 = reuters.raw('test/14826')\n",
    "print(raw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars1 = [',', '\"', ':', ')', '(', '\\n', '>', '<', ';', \"'s\", \"'\", '&']\n",
    "chars2 = ['. '] #['-'] ## Hong-kong vs far-reaching ?\n",
    "def clean_text(x):\n",
    "    for char in chars1:\n",
    "        if char in x:\n",
    "            x = x.replace(char, '')\n",
    "    for char in chars2:\n",
    "        if char in x:\n",
    "            x = x.replace(char, ' ')\n",
    "    return x\n",
    "clean1 = clean_text(raw1)\n",
    "print(clean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean1 = clean1.lower()\n",
    "print(clean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "## check if word exists\n",
    "\"would\" in words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\") ## another way\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wordnet.synsets(\"world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean1 = re.sub(' +', ' ', clean1)\n",
    "print(clean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenization\n",
    "words1 = clean1.split(\" \")\n",
    "print(words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1 = nltk.Text(words1)\n",
    "## how many words?\n",
    "len(article1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1.count(\"trade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1.common_contexts([\"japan\", \"u.s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1.dispersion_plot([\"japan\", \"australia\", 'u.s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1.concordance(\"japan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words('english')\n",
    "print(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## without stop words\n",
    "wc1 = WordCloud(stopwords=None, max_words=25, background_color=\"white\").generate(clean1)\n",
    "\n",
    "rcParams[\"figure.figsize\"] = (10,5)\n",
    "plt.imshow(wc1)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## without stop words\n",
    "wc1 = WordCloud(stopwords=None, max_words=25, background_color=\"white\").generate_from_text(clean1)\n",
    "\n",
    "rcParams[\"figure.figsize\"] = (10,5)\n",
    "plt.imshow(wc1)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add some stopwords to the current list\n",
    "cachedStopWords.append(\"said\")\n",
    "cachedStopWords.append(\"might\")\n",
    "## with stop words\n",
    "wc2 = WordCloud(stopwords=cachedStopWords, max_words=25, background_color=\"white\").generate_from_text(clean1)\n",
    "\n",
    "\n",
    "rcParams[\"figure.figsize\"] = (10,5)\n",
    "plt.imshow(wc2)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "tokens = article1[0:10]\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "print([porter.stem(t) for t in tokens])\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet') ## wordnet is a rich dictionnary with definitions and synon # import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) # v denotes verb in \"pos\"\n",
    "print(\"is :\", lemmatizer.lemmatize(\"is\", pos =\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "## \"synonym set\", a collection of synonymous words (or \"lemmas\")\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('car') ## ambiguous word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = '1'\n",
    "print(wn.synset('car.n.0'+i).definition())\n",
    "print(wn.synset('car.n.0'+i).lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## frequency distributions\n",
    "fdist = nltk.FreqDist(article1)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for most common words\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## words count\n",
    "fdist.plot(10, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## freqency of word lengths\n",
    "fdist = nltk.FreqDist(len(w) for w in article1)\n",
    "fdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import re\n",
    "\n",
    "def tokenize(text, min_length=3):\n",
    "    \"\"\"\n",
    "    A tokenizer typical used for classification\n",
    "    \"\"\"\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "    words = [word for word in words if word not in cachedStopWords]\n",
    "    tokens = (list(map(lambda token: porter.stem(token), words)))\n",
    "    p = re.compile('[a-zA-Z]+')\n",
    "    filtered_tokens = list(filter(lambda token: p.match(token) and len(token) >= min_length, tokens))\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Stock futures soared Monday morning after Moderna (MRNA) became the latest m'\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "## we take 10 documents from the Reuters dataset\n",
    "docs = [reuters.raw(doc_id) for doc_id in reuters.fileids()[:10]]\n",
    "docs_bow = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs_bow.shape)\n",
    "docs_bow.toarray()[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer2 = TfidfVectorizer(tokenizer=tokenize)\n",
    "docs_bow2 = vectorizer2.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs_bow2.shape)\n",
    "np.round(docs_bow2.toarray()[1,], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(raw1)\n",
    "print(blob.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSentimentDict(filename):\n",
    "    myfile = open(filename, \"r\")\n",
    "    lines = myfile.readlines()\n",
    "    sent = [l.split(\",\")[0].strip().lower() for l in lines]\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Loughran-McDonald_MasterDictionary_1993-2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Positive'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize ## Notice: this is a sentence tokenizer\n",
    "text = raw1\n",
    "sentences = sent_tokenize(text)\n",
    "print(len(sentences))\n",
    "##\n",
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tfidf = vectorizer2.fit_transform(sentences).toarray()\n",
    "print(sent_tfidf.shape)\n",
    "np.round(sent_tfidf, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences(x):\n",
    "    sentenceValue = {}\n",
    "    ## loop over the rows\n",
    "    for i in range(x.shape[0]):\n",
    "        total_score_per_sentence = np.sum(x[i,:])\n",
    "        count_words_in_sentence = np.sum(x[i,:] > 0)\n",
    "        sentenceValue[i] = total_score_per_sentence / count_words_in_sentence\n",
    "    return sentenceValue\n",
    "sent_scores = score_sentences(sent_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtl = 0.9\n",
    "sent_threshold = np.quantile(list(sent_scores.values()), qtl)\n",
    "print(sent_threshold)\n",
    "def generate_summary(sentences, scores, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i] >= (threshold):\n",
    "            summary += \" \\n\\n \" + sentences[i]\n",
    "            sentence_count += 1\n",
    "    return summary, sentence_count\n",
    "summary, count = generate_summary(sentences, sent_scores, sent_threshold)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "# load tagger\n",
    "classifier = TextClassifier.load('sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make example sentence\n",
    "sentence = Sentence(\"Finnish steel maker Rautaruukki Oyj ( Ruukki ) said on July 7 , 2008 that it won a 9.0 mln euro ( $ 14.1 mln ) contract to supply and install steel superstructures for Partihallsforbindelsen bridge project in Gothenburg , western Sweden.\")\n",
    "\n",
    "# call predict\n",
    "classifier.predict(sentence)\n",
    "\n",
    "# check prediction\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all cik number available (intersection between our dataframe and the data on CRSP)\n",
    "ciks_data = db.raw_sql(f'''\n",
    "        select distinct comp_cik\n",
    "        from crsp_q_mutualfunds.crsp_cik_map\n",
    "        where comp_cik in ({ciks})\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

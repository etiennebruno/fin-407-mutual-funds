{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reuters.fileids()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reuters.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reuters.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## categories overlap on the Reuters corpus\n",
    "print(reuters.categories('training/9865'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1 = reuters.raw('test/14826')\n",
    "print(raw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars1 = [',', '\"', ':', ')', '(', '\\n', '>', '<', ';', \"'s\", \"'\", '&']\n",
    "chars2 = ['. '] #['-'] ## Hong-kong vs far-reaching ?\n",
    "def clean_text(x):\n",
    "    for char in chars1:\n",
    "        if char in x:\n",
    "            x = x.replace(char, '')\n",
    "    for char in chars2:\n",
    "        if char in x:\n",
    "            x = x.replace(char, ' ')\n",
    "    return x\n",
    "clean1 = clean_text(raw1)\n",
    "print(clean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean1 = clean1.lower()\n",
    "print(clean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "## check if word exists\n",
    "\"would\" in words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\") ## another way\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wordnet.synsets(\"world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean1 = re.sub(' +', ' ', clean1)\n",
    "print(clean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenization\n",
    "words1 = clean1.split(\" \")\n",
    "print(words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1 = nltk.Text(words1)\n",
    "## how many words?\n",
    "len(article1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1.count(\"trade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1.common_contexts([\"japan\", \"u.s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1.dispersion_plot([\"japan\", \"australia\", 'u.s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1.concordance(\"japan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words('english')\n",
    "print(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## without stop words\n",
    "wc1 = WordCloud(stopwords=None, max_words=25, background_color=\"white\").generate(clean1)\n",
    "\n",
    "rcParams[\"figure.figsize\"] = (10,5)\n",
    "plt.imshow(wc1)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## without stop words\n",
    "wc1 = WordCloud(stopwords=None, max_words=25, background_color=\"white\").generate_from_text(clean1)\n",
    "\n",
    "rcParams[\"figure.figsize\"] = (10,5)\n",
    "plt.imshow(wc1)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add some stopwords to the current list\n",
    "cachedStopWords.append(\"said\")\n",
    "cachedStopWords.append(\"might\")\n",
    "## with stop words\n",
    "wc2 = WordCloud(stopwords=cachedStopWords, max_words=25, background_color=\"white\").generate_from_text(clean1)\n",
    "\n",
    "\n",
    "rcParams[\"figure.figsize\"] = (10,5)\n",
    "plt.imshow(wc2)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "tokens = article1[0:10]\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "print([porter.stem(t) for t in tokens])\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet') ## wordnet is a rich dictionnary with definitions and synon # import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) # v denotes verb in \"pos\"\n",
    "print(\"is :\", lemmatizer.lemmatize(\"is\", pos =\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "## \"synonym set\", a collection of synonymous words (or \"lemmas\")\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('car') ## ambiguous word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = '1'\n",
    "print(wn.synset('car.n.0'+i).definition())\n",
    "print(wn.synset('car.n.0'+i).lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## frequency distributions\n",
    "fdist = nltk.FreqDist(article1)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for most common words\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## words count\n",
    "fdist.plot(10, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## freqency of word lengths\n",
    "fdist = nltk.FreqDist(len(w) for w in article1)\n",
    "fdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import re\n",
    "\n",
    "def tokenize(text, min_length=3):\n",
    "    \"\"\"\n",
    "    A tokenizer typical used for classification\n",
    "    \"\"\"\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "    words = [word for word in words if word not in cachedStopWords]\n",
    "    tokens = (list(map(lambda token: porter.stem(token), words)))\n",
    "    p = re.compile('[a-zA-Z]+')\n",
    "    filtered_tokens = list(filter(lambda token: p.match(token) and len(token) >= min_length, tokens))\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Stock futures soared Monday morning after Moderna (MRNA) became the latest m'\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "## we take 10 documents from the Reuters dataset\n",
    "docs = [reuters.raw(doc_id) for doc_id in reuters.fileids()[:10]]\n",
    "docs_bow = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs_bow.shape)\n",
    "docs_bow.toarray()[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer2 = TfidfVectorizer(tokenizer=tokenize)\n",
    "docs_bow2 = vectorizer2.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs_bow2.shape)\n",
    "np.round(docs_bow2.toarray()[1,], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(raw1)\n",
    "print(blob.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSentimentDict(filename):\n",
    "    myfile = open(filename, \"r\")\n",
    "    lines = myfile.readlines()\n",
    "    sent = [l.split(\",\")[0].strip().lower() for l in lines]\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Loughran-McDonald_MasterDictionary_1993-2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Positive'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize ## Notice: this is a sentence tokenizer\n",
    "text = raw1\n",
    "sentences = sent_tokenize(text)\n",
    "print(len(sentences))\n",
    "##\n",
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tfidf = vectorizer2.fit_transform(sentences).toarray()\n",
    "print(sent_tfidf.shape)\n",
    "np.round(sent_tfidf, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences(x):\n",
    "    sentenceValue = {}\n",
    "    ## loop over the rows\n",
    "    for i in range(x.shape[0]):\n",
    "        total_score_per_sentence = np.sum(x[i,:])\n",
    "        count_words_in_sentence = np.sum(x[i,:] > 0)\n",
    "        sentenceValue[i] = total_score_per_sentence / count_words_in_sentence\n",
    "    return sentenceValue\n",
    "sent_scores = score_sentences(sent_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtl = 0.9\n",
    "sent_threshold = np.quantile(list(sent_scores.values()), qtl)\n",
    "print(sent_threshold)\n",
    "def generate_summary(sentences, scores, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i] >= (threshold):\n",
    "            summary += \" \\n\\n \" + sentences[i]\n",
    "            sentence_count += 1\n",
    "    return summary, sentence_count\n",
    "summary, count = generate_summary(sentences, sent_scores, sent_threshold)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "# load tagger\n",
    "classifier = TextClassifier.load('sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make example sentence\n",
    "sentence = Sentence(\"Finnish steel maker Rautaruukki Oyj ( Ruukki ) said on July 7 , 2008 that it won a 9.0 mln euro ( $ 14.1 mln ) contract to supply and install steel superstructures for Partihallsforbindelsen bridge project in Gothenburg , western Sweden.\")\n",
    "\n",
    "# call predict\n",
    "classifier.predict(sentence)\n",
    "\n",
    "# check prediction\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path_finbert_df = os.path.join('data', 'sentiment_analysis', 'df_finbert_predictions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle file with analysis results of FINBERT\n",
    "df = load_pkl(path_finbert_df)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv file\n",
    "# path_finbert_df_csv = os.path.join('data', 'sentiment_analysis', 'df_finbert_predictions.csv')\n",
    "# df.to_csv(path_finbert_df_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.finbert_positive.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.to_frame().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comp_cik</th>\n",
       "      <th>year</th>\n",
       "      <th>vwretd</th>\n",
       "      <th>yret</th>\n",
       "      <th>basis_points_delta</th>\n",
       "      <th>finbert_positive</th>\n",
       "      <th>finbert_negative</th>\n",
       "      <th>finbert_neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1567101</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>0.010643</td>\n",
       "      <td>0.007714</td>\n",
       "      <td>0.061879</td>\n",
       "      <td>0.124403</td>\n",
       "      <td>0.813717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1567101</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>0.010643</td>\n",
       "      <td>0.007714</td>\n",
       "      <td>0.025089</td>\n",
       "      <td>0.107892</td>\n",
       "      <td>0.867019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1567138</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.110543</td>\n",
       "      <td>0.076809</td>\n",
       "      <td>0.812648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1567138</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.135020</td>\n",
       "      <td>0.126333</td>\n",
       "      <td>0.738647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1570384</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.018772</td>\n",
       "      <td>0.030982</td>\n",
       "      <td>0.012210</td>\n",
       "      <td>0.066827</td>\n",
       "      <td>0.103726</td>\n",
       "      <td>0.829447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>1094</td>\n",
       "      <td>1710607</td>\n",
       "      <td>2018</td>\n",
       "      <td>-0.089890</td>\n",
       "      <td>-0.038793</td>\n",
       "      <td>0.051097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>1095</td>\n",
       "      <td>1710607</td>\n",
       "      <td>2018</td>\n",
       "      <td>-0.089890</td>\n",
       "      <td>-0.038793</td>\n",
       "      <td>0.051097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>1096</td>\n",
       "      <td>1722478</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>-0.004034</td>\n",
       "      <td>-0.009399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>1097</td>\n",
       "      <td>1727074</td>\n",
       "      <td>2018</td>\n",
       "      <td>-0.089890</td>\n",
       "      <td>-0.067275</td>\n",
       "      <td>0.022615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>1098</td>\n",
       "      <td>1571768</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.026159</td>\n",
       "      <td>0.011526</td>\n",
       "      <td>-0.014633</td>\n",
       "      <td>0.181580</td>\n",
       "      <td>0.094730</td>\n",
       "      <td>0.723690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1099 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  comp_cik  year    vwretd      yret  basis_points_delta  \\\n",
       "0              0   1567101  2016  0.002929  0.010643            0.007714   \n",
       "1              1   1567101  2016  0.002929  0.010643            0.007714   \n",
       "2              2   1567138  2016  0.002929  0.005900            0.002971   \n",
       "3              3   1567138  2016  0.002929  0.005900            0.002971   \n",
       "4              4   1570384  2016  0.018772  0.030982            0.012210   \n",
       "...          ...       ...   ...       ...       ...                 ...   \n",
       "1094        1094   1710607  2018 -0.089890 -0.038793            0.051097   \n",
       "1095        1095   1710607  2018 -0.089890 -0.038793            0.051097   \n",
       "1096        1096   1722478  2018  0.005365 -0.004034           -0.009399   \n",
       "1097        1097   1727074  2018 -0.089890 -0.067275            0.022615   \n",
       "1098        1098   1571768  2013  0.026159  0.011526           -0.014633   \n",
       "\n",
       "      finbert_positive  finbert_negative  finbert_neutral  \n",
       "0             0.061879          0.124403         0.813717  \n",
       "1             0.025089          0.107892         0.867019  \n",
       "2             0.110543          0.076809         0.812648  \n",
       "3             0.135020          0.126333         0.738647  \n",
       "4             0.066827          0.103726         0.829447  \n",
       "...                ...               ...              ...  \n",
       "1094               NaN               NaN              NaN  \n",
       "1095               NaN               NaN              NaN  \n",
       "1096               NaN               NaN              NaN  \n",
       "1097               NaN               NaN              NaN  \n",
       "1098          0.181580          0.094730         0.723690  \n",
       "\n",
       "[1099 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/df_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}